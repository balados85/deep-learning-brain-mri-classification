{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRIcnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balados85/deep-learning-brain-mri-classification/blob/main/MRIcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA5mEWiaJPIV"
      },
      "source": [
        "**CNN Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiSXlngbJp57"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_VjGJnWPStG",
        "outputId": "fe2ec749-ddd8-4e3c-c904-fe29493e304a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sY81cCnPU6h"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MHVWYNjOHBpL",
        "outputId": "ba346381-9cc6-4acf-ec53-a7f6e2b0fb78"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoeJ3ExlJypv"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gFaDU8VHQSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2457be60-9a99-48d3-d1d7-1d4a42611b13"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "train_set = test_datagen.flow_from_directory('drive/MyDrive/mri_keras_images/train',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 32)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1746 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxUVzuHH6TKg",
        "outputId": "9df2fc90-f354-4fe7-f34b-f26c79d5324e"
      },
      "source": [
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "validation_set = validation_datagen.flow_from_directory('drive/MyDrive/mri_keras_images/validation',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 32)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 806 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83_YnBEOHYQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93111ebf-3e4c-4700-bc84-d0d92418027d"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('drive/MyDrive/mri_keras_images/test',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 32)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 569 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW0CSMtZ5gYZ"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynREEmNcKQZN"
      },
      "source": [
        "**Bulding the CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPCkde2NHl0F"
      },
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrraphCJMp7Q"
      },
      "source": [
        "**Step 1 : Convolution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMs-wqj3HnDh"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[224, 224, 3]))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOKcMAbZM5A0"
      },
      "source": [
        "**Step 2 : Pooling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwabx6hnHtIr"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSl92U_DNKwe"
      },
      "source": [
        "**Adding a second convolution Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2LdzNYpH2LQ"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "cnn.add(tf.keras.layers.Dropout(0.25))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5KEd_9dhW_q"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Dropout(0.25))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH1n0htKNfC6"
      },
      "source": [
        "**Step 3: Flattening**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi_MJ-uyIAWj"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d24TTM0OBpL"
      },
      "source": [
        "**Step 4: Full Connection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo3nkG-iIEmf"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=320, activation='relu'))\n",
        "cnn.add(tf.keras.layers.Dropout(0.5))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJdFIAGoOQoZ"
      },
      "source": [
        "**Step 5 Output Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huKV8aLIINzJ"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=3, activation='softmax'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnAM9UarOclS"
      },
      "source": [
        "# Training the CNN:\n",
        "\n",
        "**Compiling the CNN **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtlKQ7kOJafB"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2Rn00HQITQm"
      },
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bmq39R6HO_",
        "outputId": "8734bc2d-b635-491a-9750-63cc5ba0b269"
      },
      "source": [
        "cnn.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 109, 109, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 52, 52, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 320)               1474880   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 320)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 963       \n",
            "=================================================================\n",
            "Total params: 1,504,483\n",
            "Trainable params: 1,504,483\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vxXtqNaPcWU"
      },
      "source": [
        "**Training the CNN on the training set and evaluate it on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9oGxKi5Ic30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45cec426-d3e7-439e-bce0-4c58639724af"
      },
      "source": [
        "cnn.fit(x = train_set, validation_data = validation_set, epochs = 23)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/23\n",
            "55/55 [==============================] - 1956s 36s/step - loss: 0.8980 - accuracy: 0.5590 - val_loss: 0.3049 - val_accuracy: 0.9578\n",
            "Epoch 2/23\n",
            "55/55 [==============================] - 127s 2s/step - loss: 0.2122 - accuracy: 0.9324 - val_loss: 0.0504 - val_accuracy: 0.9801\n",
            "Epoch 3/23\n",
            "55/55 [==============================] - 127s 2s/step - loss: 0.0664 - accuracy: 0.9777 - val_loss: 0.1000 - val_accuracy: 0.9826\n",
            "Epoch 4/23\n",
            "55/55 [==============================] - 128s 2s/step - loss: 0.0376 - accuracy: 0.9880 - val_loss: 0.0814 - val_accuracy: 0.9826\n",
            "Epoch 5/23\n",
            "55/55 [==============================] - 133s 2s/step - loss: 0.0449 - accuracy: 0.9880 - val_loss: 0.1628 - val_accuracy: 0.9677\n",
            "Epoch 6/23\n",
            "55/55 [==============================] - 131s 2s/step - loss: 0.0321 - accuracy: 0.9908 - val_loss: 0.0968 - val_accuracy: 0.9789\n",
            "Epoch 7/23\n",
            "55/55 [==============================] - 130s 2s/step - loss: 0.0216 - accuracy: 0.9943 - val_loss: 0.1666 - val_accuracy: 0.9777\n",
            "Epoch 8/23\n",
            "55/55 [==============================] - 129s 2s/step - loss: 0.0083 - accuracy: 0.9983 - val_loss: 0.2292 - val_accuracy: 0.9727\n",
            "Epoch 9/23\n",
            "55/55 [==============================] - 134s 2s/step - loss: 0.0107 - accuracy: 0.9971 - val_loss: 0.2285 - val_accuracy: 0.9727\n",
            "Epoch 10/23\n",
            "55/55 [==============================] - 132s 2s/step - loss: 0.0090 - accuracy: 0.9954 - val_loss: 0.1649 - val_accuracy: 0.9715\n",
            "Epoch 11/23\n",
            "55/55 [==============================] - 131s 2s/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.2092 - val_accuracy: 0.9752\n",
            "Epoch 12/23\n",
            "55/55 [==============================] - 131s 2s/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.2696 - val_accuracy: 0.9777\n",
            "Epoch 13/23\n",
            "55/55 [==============================] - 133s 2s/step - loss: 0.0135 - accuracy: 0.9966 - val_loss: 0.2208 - val_accuracy: 0.9677\n",
            "Epoch 14/23\n",
            "55/55 [==============================] - 133s 2s/step - loss: 0.0024 - accuracy: 0.9989 - val_loss: 0.2092 - val_accuracy: 0.9826\n",
            "Epoch 15/23\n",
            "55/55 [==============================] - 132s 2s/step - loss: 0.0085 - accuracy: 0.9966 - val_loss: 0.2259 - val_accuracy: 0.9715\n",
            "Epoch 16/23\n",
            "55/55 [==============================] - 136s 2s/step - loss: 0.0083 - accuracy: 0.9954 - val_loss: 0.2137 - val_accuracy: 0.9702\n",
            "Epoch 17/23\n",
            "55/55 [==============================] - 130s 2s/step - loss: 0.0176 - accuracy: 0.9948 - val_loss: 0.2705 - val_accuracy: 0.9727\n",
            "Epoch 18/23\n",
            "55/55 [==============================] - 131s 2s/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.2435 - val_accuracy: 0.9814\n",
            "Epoch 19/23\n",
            "55/55 [==============================] - 131s 2s/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.2971 - val_accuracy: 0.9764\n",
            "Epoch 20/23\n",
            "55/55 [==============================] - 131s 2s/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.2105 - val_accuracy: 0.9839\n",
            "Epoch 21/23\n",
            "55/55 [==============================] - 130s 2s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2927 - val_accuracy: 0.9777\n",
            "Epoch 22/23\n",
            "55/55 [==============================] - 131s 2s/step - loss: 8.3982e-04 - accuracy: 1.0000 - val_loss: 0.2335 - val_accuracy: 0.9777\n",
            "Epoch 23/23\n",
            "55/55 [==============================] - 131s 2s/step - loss: 4.5035e-04 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f256213a550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-CJkdESP5zm"
      },
      "source": [
        "# Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkSic8CFIpPo"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('drive/MyDrive/mri_keras_images/test/2/1010.png', target_size = (224, 224))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "train_set.class_indices\n",
        "if result[0][0] ==0:\n",
        "        prediction = 'glioma'\n",
        "elif result[0][0] ==1:\n",
        "       prediction = 'meningioma'\n",
        "elif result[0][0] ==2:\n",
        "       prediction = 'pituitary'\n",
        "else:\n",
        "  prediction = 'not Tumor'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjmAhQpxI5ZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacf90c9-334f-4721-ac08-67eaf477bd1f"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glioma\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}